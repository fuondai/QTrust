# QTrust: TÃ i liá»‡u API vÃ  HÆ°á»›ng dáº«n

<div align="center">
  <img src="models/training_metrics.png" alt="QTrust Performance Metrics" width="600">
</div>

TÃ i liá»‡u nÃ y mÃ´ táº£ chi tiáº¿t cÃ¡c API vÃ  thÃ nh pháº§n cá»§a há»‡ thá»‘ng QTrust, giÃºp nhÃ  phÃ¡t triá»ƒn hiá»ƒu vÃ  má»Ÿ rá»™ng há»‡ thá»‘ng.

## ğŸ“š Má»¥c lá»¥c

- [Tá»•ng quan vá» kiáº¿n trÃºc](#tá»•ng-quan-vá»-kiáº¿n-trÃºc)
- [CÃ¡c module chÃ­nh](#cÃ¡c-module-chÃ­nh)
  - [BlockchainEnvironment](#1-blockchainenvironment)
  - [DQNAgent](#2-dqnagent)
  - [AdaptiveConsensus](#3-adaptiveconsensus)
  - [MADRAPIDRouter](#4-madrapidrouter)
  - [HTDCM](#5-htdcm-hierarchical-trust-data-center-mechanism)
  - [FederatedLearning](#6-federatedlearning)
- [Utilities](#utilities)
- [Luá»“ng dá»¯ liá»‡u](#luá»“ng-dá»¯-liá»‡u)
- [HÆ°á»›ng dáº«n nÃ¢ng cao](#hÆ°á»›ng-dáº«n-nÃ¢ng-cao)
  - [TÃ¹y chá»‰nh Agent](#tÃ¹y-chá»‰nh-agent)
  - [TÃ¹y chá»‰nh mÃ´i trÆ°á»ng](#tÃ¹y-chá»‰nh-mÃ´i-trÆ°á»ng)
  - [Táº¡o mÃ´ phá»ng tÃ¹y chá»‰nh](#táº¡o-mÃ´-phá»ng-tÃ¹y-chá»‰nh)
- [PhÃ¢n tÃ­ch káº¿t quáº£](#phÃ¢n-tÃ­ch-káº¿t-quáº£)
- [Trá»±c quan hÃ³a](#trá»±c-quan-hÃ³a)

## Tá»•ng quan vá» kiáº¿n trÃºc

QTrust bao gá»“m nhiá»u module phá»‘i há»£p vá»›i nhau Ä‘á»ƒ táº¡o thÃ nh má»™t há»‡ thá»‘ng tá»‘i Æ°u hÃ³a blockchain sharding toÃ n diá»‡n:

<div align="center">
```
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚  BlockchainEnv    â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AdaptiveConsensusâ”‚â—„â”€â”€â”€â”€â”¤     DQNAgent      â”œâ”€â”€â”€â”€â–ºâ”‚    MADRAPIDRouter â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚FederatedLearning  â”‚â—„â”€â”€â”€â”€â”¤      HTDCM        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</div>

### Luá»“ng dá»¯ liá»‡u vÃ  tÆ°Æ¡ng tÃ¡c

1. **BlockchainEnvironment** cung cáº¥p tráº¡ng thÃ¡i máº¡ng cho **DQNAgent**
2. **DQNAgent** ra quyáº¿t Ä‘á»‹nh vá» Ä‘á»‹nh tuyáº¿n vÃ  giao thá»©c Ä‘á»“ng thuáº­n
3. **AdaptiveConsensus** Ã¡p dá»¥ng giao thá»©c Ä‘á»“ng thuáº­n thÃ­ch há»£p
4. **MADRAPIDRouter** xá»­ lÃ½ Ä‘á»‹nh tuyáº¿n giao dá»‹ch dá»±a trÃªn quyáº¿t Ä‘á»‹nh cá»§a agent
5. **HTDCM** Ä‘Ã¡nh giÃ¡ Ä‘á»™ tin cáº­y cá»§a cÃ¡c nÃºt máº¡ng
6. **FederatedLearning** há»— trá»£ phÃ¢n phá»‘i kiáº¿n thá»©c giá»¯a cÃ¡c agent

## CÃ¡c module chÃ­nh

### 1. BlockchainEnvironment

MÃ´i trÆ°á»ng mÃ´ phá»ng blockchain vá»›i sharding, cung cáº¥p giao diá»‡n tÆ°Æ¡ng tÃ¡c giá»¯a cÃ¡c agent vÃ  máº¡ng blockchain.

#### Class: BlockchainEnvironment

```python
from qtrust.simulation.blockchain_environment import BlockchainEnvironment

# Táº¡o mÃ´i trÆ°á»ng
env = BlockchainEnvironment(
    num_shards=4,                   # Sá»‘ lÆ°á»£ng shard
    num_nodes_per_shard=10,         # Sá»‘ nÃºt má»—i shard
    max_transactions_per_step=100,  # Sá»‘ giao dá»‹ch tá»‘i Ä‘a má»—i bÆ°á»›c
    transaction_value_range=(0.1, 100.0),  # Pháº¡m vi giÃ¡ trá»‹ giao dá»‹ch
    max_steps=1000,                 # Sá»‘ bÆ°á»›c tá»‘i Ä‘a má»—i episode
    latency_penalty=0.5,            # Há»‡ sá»‘ pháº¡t Ä‘á»™ trá»…
    energy_penalty=0.3,             # Há»‡ sá»‘ pháº¡t nÄƒng lÆ°á»£ng
    throughput_reward=1.0,          # Há»‡ sá»‘ thÆ°á»Ÿng thÃ´ng lÆ°á»£ng
    security_reward=0.8             # Há»‡ sá»‘ thÆ°á»Ÿng báº£o máº­t
)

# Reset mÃ´i trÆ°á»ng
state = env.reset()

# Thá»±c hiá»‡n má»™t bÆ°á»›c
action = [1, 0]  # [shard_id, consensus_protocol]
next_state, reward, done, info = env.step(action)

# Láº¥y thÃ´ng tin vá» mÃ´i trÆ°á»ng
print(f"Sá»‘ shard: {env.num_shards}")
print(f"Tá»•ng sá»‘ nÃºt: {env.total_nodes}")
print(f"ThÃ´ng lÆ°á»£ng hiá»‡n táº¡i: {env.current_throughput} tx/s")
print(f"Äá»™ trá»… trung bÃ¬nh: {env.average_latency} ms")
```

#### KhÃ´ng gian tráº¡ng thÃ¡i

MÃ´i trÆ°á»ng sá»­ dá»¥ng khÃ´ng gian tráº¡ng thÃ¡i cÃ³ kÃ­ch thÆ°á»›c `num_shards * 4 + 3 + 3`:
- 4 Ä‘áº·c trÆ°ng cho má»—i shard:
  - Má»©c Ä‘á»™ táº¯c ngháº½n (0-1)
  - GiÃ¡ trá»‹ giao dá»‹ch trung bÃ¬nh
  - Äiá»ƒm tin cáº­y trung bÃ¬nh cá»§a shard
  - Tá»· lá»‡ giao dá»‹ch thÃ nh cÃ´ng
- 3 Ä‘áº·c trÆ°ng toÃ n cá»¥c:
  - Sá»‘ giao dá»‹ch Ä‘ang chá»
  - Äá»™ trá»… trung bÃ¬nh
  - Tá»· lá»‡ giao dá»‹ch xuyÃªn shard
- 3 pháº§n tá»­ cho tá»· lá»‡ Ä‘á»“ng thuáº­n hiá»‡n táº¡i

#### KhÃ´ng gian hÃ nh Ä‘á»™ng

KhÃ´ng gian hÃ nh Ä‘á»™ng bao gá»“m hai chiá»u:
- Lá»±a chá»n shard Ä‘Ã­ch (0 Ä‘áº¿n num_shards-1)
- Lá»±a chá»n giao thá»©c Ä‘á»“ng thuáº­n (0: Fast BFT, 1: PBFT, 2: Robust BFT)

#### PhÆ°Æ¡ng thá»©c quan trá»ng

| PhÆ°Æ¡ng thá»©c | MÃ´ táº£ |
|-------------|-------|
| `reset()` | Khá»Ÿi táº¡o láº¡i mÃ´i trÆ°á»ng vá» tráº¡ng thÃ¡i ban Ä‘áº§u |
| `step(action)` | Thá»±c hiá»‡n hÃ nh Ä‘á»™ng vÃ  tráº£ vá» state má»›i, reward, done, info |
| `render()` | Hiá»ƒn thá»‹ tráº¡ng thÃ¡i hiá»‡n táº¡i cá»§a mÃ´i trÆ°á»ng |
| `get_state()` | Láº¥y vector tráº¡ng thÃ¡i hiá»‡n táº¡i |
| `get_reward(action)` | TÃ­nh toÃ¡n pháº§n thÆ°á»Ÿng cho hÃ nh Ä‘á»™ng |
| `is_terminal()` | Kiá»ƒm tra xem episode Ä‘Ã£ káº¿t thÃºc chÆ°a |

### 2. DQNAgent

Agent há»c tÄƒng cÆ°á»ng sÃ¢u cho viá»‡c ra quyáº¿t Ä‘á»‹nh trong mÃ´i trÆ°á»ng blockchain.

#### Class: DQNAgent

```python
from qtrust.agents.dqn_agent import DQNAgent, QNetwork

# Táº¡o agent
agent = DQNAgent(
    state_space=state_space,            # KÃ­ch thÆ°á»›c khÃ´ng gian tráº¡ng thÃ¡i
    action_space=action_space,          # KÃ­ch thÆ°á»›c khÃ´ng gian hÃ nh Ä‘á»™ng
    num_shards=4,                       # Sá»‘ lÆ°á»£ng shard
    learning_rate=0.001,                # Tá»‘c Ä‘á»™ há»c
    gamma=0.99,                         # Há»‡ sá»‘ giáº£m giÃ¡
    epsilon_start=1.0,                  # Epsilon ban Ä‘áº§u cho khÃ¡m phÃ¡
    epsilon_end=0.01,                   # Epsilon tá»‘i thiá»ƒu
    epsilon_decay=0.995,                # Tá»‘c Ä‘á»™ giáº£m epsilon
    buffer_size=10000,                  # KÃ­ch thÆ°á»›c bá»™ nhá»› kinh nghiá»‡m
    batch_size=64,                      # KÃ­ch thÆ°á»›c batch
    update_target_every=100             # Cáº­p nháº­t máº¡ng Ä‘Ã­ch sau bao nhiÃªu bÆ°á»›c
)

# Chá»n hÃ nh Ä‘á»™ng tá»« tráº¡ng thÃ¡i
state = env.reset()
action = agent.act(state)

# Há»c tá»« kinh nghiá»‡m
next_state, reward, done, _ = env.step(action)
agent.step(state, action, reward, next_state, done)

# Huáº¥n luyá»‡n agent
scores = agent.train(env, num_episodes=500, max_steps=1000)

# LÆ°u vÃ  táº£i mÃ´ hÃ¬nh
agent.save('models/dqn_model.pth')
agent.load('models/dqn_model.pth')

# ÄÃ¡nh giÃ¡ agent
eval_scores = agent.evaluate(env, num_episodes=10)
print(f"Äiá»ƒm trung bÃ¬nh: {np.mean(eval_scores)}")
```

#### Kiáº¿n trÃºc QNetwork

QNetwork lÃ  máº¡ng neural sá»­ dá»¥ng trong DQNAgent vá»›i kiáº¿n trÃºc sau:

```python
class QNetwork(nn.Module):
    def __init__(self, state_size, action_dims, hidden_sizes=[64, 64]):
        super(QNetwork, self).__init__()
        self.state_size = state_size
        self.action_dims = action_dims
        
        # Táº§ng Ä‘áº§u vÃ o
        layers = [nn.Linear(state_size, hidden_sizes[0]),
                  nn.ReLU()]
        
        # Táº§ng áº©n
        for i in range(len(hidden_sizes)-1):
            layers.extend([
                nn.Linear(hidden_sizes[i], hidden_sizes[i+1]),
                nn.ReLU()
            ])
            
        self.shared_layers = nn.Sequential(*layers)
        
        # Táº§ng ra quyáº¿t Ä‘á»‹nh shard
        self.shard_head = nn.Linear(hidden_sizes[-1], action_dims[0])
        
        # Táº§ng ra quyáº¿t Ä‘á»‹nh giao thá»©c
        self.protocol_head = nn.Linear(hidden_sizes[-1], action_dims[1])
        
    def forward(self, state):
        x = self.shared_layers(state)
        shard_values = self.shard_head(x)
        protocol_values = self.protocol_head(x)
        return shard_values, protocol_values
```

#### PhÆ°Æ¡ng thá»©c quan trá»ng

| PhÆ°Æ¡ng thá»©c | MÃ´ táº£ |
|-------------|-------|
| `act(state)` | Chá»n hÃ nh Ä‘á»™ng dá»±a trÃªn tráº¡ng thÃ¡i hiá»‡n táº¡i |
| `step(state, action, reward, next_state, done)` | LÆ°u kinh nghiá»‡m vÃ  há»c náº¿u Ä‘á»§ Ä‘iá»u kiá»‡n |
| `learn()` | Thá»±c hiá»‡n quÃ¡ trÃ¬nh há»c tá»« bá»™ nhá»› kinh nghiá»‡m |
| `train(env, num_episodes, max_steps)` | Huáº¥n luyá»‡n agent trÃªn mÃ´i trÆ°á»ng |
| `evaluate(env, num_episodes)` | ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t cá»§a agent |
| `save(path)` | LÆ°u mÃ´ hÃ¬nh |
| `load(path)` | Táº£i mÃ´ hÃ¬nh Ä‘Ã£ lÆ°u |

### 3. AdaptiveConsensus

Chuyá»ƒn Ä‘á»•i Ä‘á»™ng giá»¯a cÃ¡c giao thá»©c Ä‘á»“ng thuáº­n dá»±a trÃªn Ä‘iá»u kiá»‡n máº¡ng.

#### Class: AdaptiveConsensus

```python
from qtrust.consensus.adaptive_consensus import AdaptiveConsensus, ConsensusProtocol

# Táº¡o adaptive consensus
consensus = AdaptiveConsensus(
    num_shards=4,                   # Sá»‘ lÆ°á»£ng shard
    protocols=['fast_bft', 'pbft', 'robust_bft']  # CÃ¡c giao thá»©c há»— trá»£
)

# Chá»n protocol phÃ¹ há»£p
protocol = consensus.select_protocol(
    shard_id=1,                     # ID cá»§a shard
    network_congestion=0.2,         # Má»©c Ä‘á»™ táº¯c ngháº½n máº¡ng
    trust_score=0.8,                # Äiá»ƒm tin cáº­y
    transaction_value=50.0          # GiÃ¡ trá»‹ giao dá»‹ch
)

# Ãp dá»¥ng giao thá»©c vÃ  nháº­n káº¿t quáº£
result = consensus.apply_protocol(
    shard_id=1,                     # ID cá»§a shard
    protocol=protocol,              # Giao thá»©c Ä‘Ã£ chá»n
    transactions=tx_list            # Danh sÃ¡ch giao dá»‹ch
)

# Láº¥y thÃ´ng tin hiá»‡u suáº¥t
latency = consensus.get_protocol_latency(protocol, network_congestion)
energy = consensus.get_protocol_energy_consumption(protocol)
security = consensus.get_protocol_security_level(protocol, trust_score)
```

#### CÃ¡c giao thá»©c Ä‘á»“ng thuáº­n

| Giao thá»©c | MÃ´ táº£ | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|-----------|-------|---------|------------|
| Fast BFT | BFT nhanh, tá»‘i Æ°u cho máº¡ng Ã­t táº¯c ngháº½n | Äá»™ trá»… tháº¥p, tiÃªu thá»¥ nÄƒng lÆ°á»£ng tháº¥p | Báº£o máº­t tháº¥p hÆ¡n |
| PBFT | Practical Byzantine Fault Tolerance | CÃ¢n báº±ng tá»‘t giá»¯a hiá»‡u suáº¥t vÃ  báº£o máº­t | TiÃªu thá»¥ nhiá»u tÃ i nguyÃªn hÆ¡n |
| Robust BFT | BFT tÄƒng cÆ°á»ng báº£o máº­t | Báº£o máº­t cao nháº¥t | Äá»™ trá»… cao, tiÃªu thá»¥ nhiá»u nÄƒng lÆ°á»£ng |

#### PhÆ°Æ¡ng thá»©c quan trá»ng

| PhÆ°Æ¡ng thá»©c | MÃ´ táº£ |
|-------------|-------|
| `select_protocol(...)` | Chá»n giao thá»©c Ä‘á»“ng thuáº­n phÃ¹ há»£p vá»›i Ä‘iá»u kiá»‡n |
| `apply_protocol(...)` | Ãp dá»¥ng giao thá»©c Ä‘Ã£ chá»n cho cÃ¡c giao dá»‹ch |
| `get_protocol_latency(...)` | Æ¯á»›c tÃ­nh Ä‘á»™ trá»… cho giao thá»©c |
| `get_protocol_energy_consumption(...)` | Æ¯á»›c tÃ­nh má»©c tiÃªu thá»¥ nÄƒng lÆ°á»£ng |
| `get_protocol_security_level(...)` | Æ¯á»›c tÃ­nh má»©c Ä‘á»™ báº£o máº­t |
| `get_consensus_distribution()` | Láº¥y phÃ¢n phá»‘i hiá»‡n táº¡i cá»§a cÃ¡c giao thá»©c |

### 4. MADRAPIDRouter

Há»‡ thá»‘ng Ä‘á»‹nh tuyáº¿n giao dá»‹ch thÃ´ng minh Ä‘a tÃ¡c tá»­.

#### Class: MADRAPIDRouter

```python
from qtrust.routing.mad_rapid import MADRAPIDRouter

# Táº¡o router
router = MADRAPIDRouter(
    num_shards=4,                   # Sá»‘ lÆ°á»£ng shard
    network_topology=network_graph  # Topology máº¡ng (NetworkX graph)
)

# Äá»‹nh tuyáº¿n giao dá»‹ch
route = router.route_transaction(
    transaction=tx,                 # Giao dá»‹ch cáº§n Ä‘á»‹nh tuyáº¿n
    current_shard=0,                # Shard hiá»‡n táº¡i
    congestion_levels=[0.2, 0.5, 0.8, 0.3]  # Má»©c Ä‘á»™ táº¯c ngháº½n cá»§a cÃ¡c shard
)

# Láº¥y thÃ´ng tin vá» Ä‘Æ°á»ng Ä‘i vÃ  hops
path, num_hops = router.get_path_info(source_shard=0, destination_shard=2)

# PhÃ¢n tÃ­ch hiá»‡u suáº¥t Ä‘á»‹nh tuyáº¿n
metrics = router.analyze_routing_performance(transactions=txs, paths=paths)
print(f"Äá»™ trá»… trung bÃ¬nh: {metrics['avg_latency']} ms")
print(f"Sá»‘ hop trung bÃ¬nh: {metrics['avg_hops']}")
print(f"Tá»· lá»‡ thÃ nh cÃ´ng: {metrics['success_rate']}")
```

#### Thuáº­t toÃ¡n Ä‘á»‹nh tuyáº¿n

MADRAPIDRouter sá»­ dá»¥ng káº¿t há»£p cÃ¡c thuáº­t toÃ¡n sau:
1. **Q-Learning**: Há»c tá»« kinh nghiá»‡m Ä‘á»‹nh tuyáº¿n trÆ°á»›c Ä‘Ã³
2. **Dijkstra's**: TÃ¬m Ä‘Æ°á»ng Ä‘i ngáº¯n nháº¥t trong máº¡ng
3. **Ant Colony Optimization**: Tá»‘i Æ°u hÃ³a Ä‘Æ°á»ng Ä‘i dá»±a trÃªn "mÃ¹i" cá»§a Ä‘Æ°á»ng Ä‘i thÃ nh cÃ´ng
4. **Congestion Prediction**: Dá»± Ä‘oÃ¡n táº¯c ngháº½n dá»±a trÃªn dá»¯ liá»‡u lá»‹ch sá»­

#### PhÆ°Æ¡ng thá»©c quan trá»ng

| PhÆ°Æ¡ng thá»©c | MÃ´ táº£ |
|-------------|-------|
| `route_transaction(...)` | Äá»‹nh tuyáº¿n giao dá»‹ch Ä‘áº¿n shard phÃ¹ há»£p |
| `get_path_info(...)` | Láº¥y thÃ´ng tin vá» Ä‘Æ°á»ng Ä‘i giá»¯a cÃ¡c shard |
| `update_network_topology(...)` | Cáº­p nháº­t topology máº¡ng |
| `predict_congestion(...)` | Dá»± Ä‘oÃ¡n má»©c Ä‘á»™ táº¯c ngháº½n trong tÆ°Æ¡ng lai |
| `analyze_routing_performance(...)` | PhÃ¢n tÃ­ch hiá»‡u suáº¥t Ä‘á»‹nh tuyáº¿n |

### 5. HTDCM (Hierarchical Trust Data Center Mechanism)

CÆ¡ cháº¿ Ä‘Ã¡nh giÃ¡ vÃ  quáº£n lÃ½ tin cáº­y phÃ¢n cáº¥p.

#### Class: HTDCM

```python
from qtrust.trust.htdcm import HTDCM

# Táº¡o trust mechanism
trust_mechanism = HTDCM(
    num_nodes=40,                   # Tá»•ng sá»‘ nÃºt
    num_shards=4,                   # Sá»‘ lÆ°á»£ng shard
    initial_trust=0.5,              # Äiá»ƒm tin cáº­y ban Ä‘áº§u
    alpha=0.8,                      # Há»‡ sá»‘ há»c
    detection_threshold=0.3         # NgÆ°á»¡ng phÃ¡t hiá»‡n nÃºt Ä‘á»™c háº¡i
)

# Cáº­p nháº­t Ä‘iá»ƒm tin cáº­y cho node
trust_mechanism.update_trust_score(
    node_id=5,                      # ID cá»§a nÃºt
    transaction_success=True,       # Giao dá»‹ch thÃ nh cÃ´ng?
    response_time=0.2               # Thá»i gian pháº£n há»“i
)

# PhÃ¡t hiá»‡n nÃºt Ä‘á»™c háº¡i
malicious_nodes = trust_mechanism.detect_malicious_nodes(
    threshold=0.3                   # NgÆ°á»¡ng tin cáº­y
)

# Láº¥y trust scores
shard_trust = trust_mechanism.get_shard_trust_scores()
node_trust = trust_mechanism.get_node_trust_scores()

# PhÃ¢n tÃ­ch táº¥n cÃ´ng
attack_detected = trust_mechanism.analyze_attack_patterns(recent_transactions)
if attack_detected:
    attack_type = trust_mechanism.identify_attack_type()
    print(f"PhÃ¡t hiá»‡n táº¥n cÃ´ng: {attack_type}")
```

#### CÃ¡c má»©c Ä‘Ã¡nh giÃ¡ tin cáº­y

HTDCM hoáº¡t Ä‘á»™ng trÃªn 3 má»©c:
1. **Node-level**: ÄÃ¡nh giÃ¡ tá»«ng nÃºt dá»±a trÃªn hÃ nh vi vÃ  hiá»‡u suáº¥t
2. **Shard-level**: ÄÃ¡nh giÃ¡ má»©c Ä‘á»™ tin cáº­y cá»§a toÃ n bá»™ shard
3. **Network-level**: PhÃ¡t hiá»‡n máº«u táº¥n cÃ´ng vÃ  báº¥t thÆ°á»ng trong máº¡ng

#### PhÆ°Æ¡ng thá»©c quan trá»ng

| PhÆ°Æ¡ng thá»©c | MÃ´ táº£ |
|-------------|-------|
| `update_trust_score(...)` | Cáº­p nháº­t Ä‘iá»ƒm tin cáº­y cá»§a nÃºt |
| `detect_malicious_nodes(...)` | PhÃ¡t hiá»‡n nÃºt Ä‘á»™c háº¡i |
| `get_shard_trust_scores()` | Láº¥y Ä‘iá»ƒm tin cáº­y cá»§a cÃ¡c shard |
| `get_node_trust_scores()` | Láº¥y Ä‘iá»ƒm tin cáº­y cá»§a cÃ¡c nÃºt |
| `analyze_attack_patterns(...)` | PhÃ¢n tÃ­ch máº«u táº¥n cÃ´ng |
| `identify_attack_type()` | XÃ¡c Ä‘á»‹nh loáº¡i táº¥n cÃ´ng |
| `isolate_malicious_nodes(...)` | CÃ´ láº­p cÃ¡c nÃºt Ä‘á»™c háº¡i |

### 6. FederatedLearning

Há»c táº­p liÃªn há»£p cho cÃ¡c agent DQN phÃ¢n tÃ¡n.

#### Class: FederatedLearning

```python
from qtrust.federated.federated_learning import FederatedLearning, FederatedModel, FederatedClient

# Táº¡o mÃ´ hÃ¬nh liÃªn há»£p
federated_model = FederatedModel(
    state_size=20,                  # KÃ­ch thÆ°á»›c tráº¡ng thÃ¡i
    action_dim=[4, 3],              # KÃ­ch thÆ°á»›c hÃ nh Ä‘á»™ng
    hidden_sizes=[128, 128]         # KÃ­ch thÆ°á»›c cÃ¡c táº§ng áº©n
)

# Táº¡o client liÃªn há»£p
clients = [
    FederatedClient(agent=agent1, node_id=1),
    FederatedClient(agent=agent2, node_id=2),
    # ...
]

# Táº¡o há»‡ thá»‘ng há»c táº­p liÃªn há»£p
federated_system = FederatedLearning(
    model=federated_model,          # MÃ´ hÃ¬nh liÃªn há»£p
    clients=clients,                # CÃ¡c client tham gia
    rounds=10,                      # Sá»‘ vÃ²ng há»c táº­p
    local_epochs=5,                 # Sá»‘ epoch há»c táº­p cá»¥c bá»™
    min_clients=3,                  # Sá»‘ client tá»‘i thiá»ƒu cáº§n tham gia
    privacy_level='differential'    # PhÆ°Æ¡ng phÃ¡p báº£o vá»‡ quyá»n riÃªng tÆ°
)

# Huáº¥n luyá»‡n
federated_system.train()

# ÄÃ¡nh giÃ¡
global_performance = federated_system.evaluate()
print(f"Hiá»‡u suáº¥t global: {global_performance}")

# Cáº­p nháº­t mÃ´ hÃ¬nh cho client má»›i
federated_system.update_client_model(new_client)
```

#### Luá»“ng há»c táº­p liÃªn há»£p

1. Server phÃ¢n phá»‘i mÃ´ hÃ¬nh global Ä‘áº¿n cÃ¡c client
2. Má»—i client huáº¥n luyá»‡n cá»¥c bá»™ trÃªn dá»¯ liá»‡u riÃªng
3. Client gá»­i láº¡i cÃ¡c thÃ´ng sá»‘ Ä‘Ã£ cáº­p nháº­t lÃªn server
4. Server tá»•ng há»£p thÃ´ng sá»‘ tá»« cÃ¡c client
5. Server cáº­p nháº­t mÃ´ hÃ¬nh global
6. QuÃ¡ trÃ¬nh láº·p láº¡i tá»« bÆ°á»›c 1

#### PhÆ°Æ¡ng thá»©c quan trá»ng

| PhÆ°Æ¡ng thá»©c | MÃ´ táº£ |
|-------------|-------|
| `train()` | Huáº¥n luyá»‡n mÃ´ hÃ¬nh liÃªn há»£p |
| `aggregate_models(...)` | Tá»•ng há»£p mÃ´ hÃ¬nh tá»« cÃ¡c client |
| `distribute_model()` | PhÃ¢n phá»‘i mÃ´ hÃ¬nh global Ä‘áº¿n cÃ¡c client |
| `evaluate()` | ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh global |
| `add_client(...)` | ThÃªm client má»›i vÃ o há»‡ thá»‘ng |
| `update_client_model(...)` | Cáº­p nháº­t mÃ´ hÃ¬nh cho client cá»¥ thá»ƒ |

## Utilities

### Module: data_generation

```python
from qtrust.utils.data_generation import (
    generate_network_topology,
    assign_nodes_to_shards,
    generate_transactions
)

# Táº¡o topology máº¡ng
network = generate_network_topology(
    num_nodes=40,                   # Sá»‘ lÆ°á»£ng nÃºt
    connection_probability=0.3      # XÃ¡c suáº¥t káº¿t ná»‘i
)

# PhÃ¢n phá»‘i node vÃ o cÃ¡c shard
shards = assign_nodes_to_shards(
    network=network,                # Topology máº¡ng
    num_shards=4,                   # Sá»‘ lÆ°á»£ng shard
    strategy='balanced'             # Chiáº¿n lÆ°á»£c phÃ¢n phá»‘i
)

# Táº¡o giao dá»‹ch
transactions = generate_transactions(
    num_transactions=100,           # Sá»‘ lÆ°á»£ng giao dá»‹ch
    value_range=(0.1, 100.0),       # Pháº¡m vi giÃ¡ trá»‹
    cross_shard_probability=0.3     # XÃ¡c suáº¥t giao dá»‹ch xuyÃªn shard
)
```

### Module: metrics

```python
from qtrust.utils.metrics import (
    calculate_throughput,
    calculate_latency,
    calculate_security_score,
    calculate_energy_consumption,
    evaluate_overall_performance
)

# TÃ­nh toÃ¡n cÃ¡c chá»‰ sá»‘ hiá»‡u suáº¥t
throughput = calculate_throughput(transactions_processed, time_elapsed)
latency = calculate_latency(transaction_times)
security = calculate_security_score(consensus_distribution, trust_scores)
energy = calculate_energy_consumption(consensus_distribution, num_nodes)

# ÄÃ¡nh giÃ¡ tá»•ng thá»ƒ
performance = evaluate_overall_performance(
    throughput=throughput,
    latency=latency,
    security=security,
    energy=energy,
    weights={
        'throughput': 0.3,
        'latency': 0.3,
        'security': 0.2,
        'energy': 0.2
    }
)
```

## Luá»“ng dá»¯ liá»‡u

SÆ¡ Ä‘á»“ dÆ°á»›i Ä‘Ã¢y minh há»a luá»“ng dá»¯ liá»‡u trong há»‡ thá»‘ng QTrust:

<div align="center">
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Transactions  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º    DQNAgent     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  Route Decision  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                             â”‚                            â”‚
        â”‚                             â”‚                            â”‚
        â–¼                             â–¼                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BlockchainState â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  Reward Signal  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  Performance    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                                         â–²
        â”‚                                                         â”‚
        â–¼                                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Trust Updates  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  Protocol Select â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Transaction Exec â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</div>

## HÆ°á»›ng dáº«n nÃ¢ng cao

### TÃ¹y chá»‰nh Agent

Äá»ƒ táº¡o Agent tÃ¹y chá»‰nh, báº¡n cÃ³ thá»ƒ má»Ÿ rá»™ng tá»« lá»›p cÆ¡ sá»Ÿ:

```python
from qtrust.agents.base_agent import BaseAgent
from qtrust.agents.dqn_agent import QNetwork
import torch.optim as optim

class CustomAgent(BaseAgent):
    def __init__(self, state_space, action_space, **kwargs):
        super().__init__(state_space, action_space)
        # Khá»Ÿi táº¡o tÃ¹y chá»‰nh
        self.network = QNetwork(state_space, action_space, hidden_sizes=[256, 128])
        self.optimizer = optim.Adam(self.network.parameters(), lr=kwargs.get('learning_rate', 0.001))
        
    def act(self, state):
        # Lá»±a chá»n hÃ nh Ä‘á»™ng
        pass
    
    def learn(self):
        # CÃ i Ä‘áº·t thuáº­t toÃ¡n há»c
        pass
```

### TÃ¹y chá»‰nh mÃ´i trÆ°á»ng

MÃ´i trÆ°á»ng tÃ¹y chá»‰nh cÃ³ thá»ƒ Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng cÃ¡ch má»Ÿ rá»™ng lá»›p BlockchainEnvironment:

```python
from qtrust.simulation.blockchain_environment import BlockchainEnvironment

class CustomEnvironment(BlockchainEnvironment):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # ThÃªm cÃ¡c thÃ´ng sá»‘ tÃ¹y chá»‰nh
        self.custom_parameter = kwargs.get('custom_parameter', 0)
    
    def reset(self):
        # Reset mÃ´i trÆ°á»ng vá»›i logic tÃ¹y chá»‰nh
        state = super().reset()
        # ThÃªm logic tÃ¹y chá»‰nh
        return state
    
    def step(self, action):
        # Logic tÃ¹y chá»‰nh cho bÆ°á»›c mÃ´i trÆ°á»ng
        return super().step(action)
    
    def custom_method(self):
        # PhÆ°Æ¡ng thá»©c tÃ¹y chá»‰nh
        pass
```

### Táº¡o mÃ´ phá»ng tÃ¹y chá»‰nh

Báº¡n cÃ³ thá»ƒ táº¡o mÃ´ phá»ng tÃ¹y chá»‰nh báº±ng cÃ¡ch tÃ­ch há»£p cÃ¡c thÃ nh pháº§n:

```python
# Táº¡o mÃ´i trÆ°á»ng tÃ¹y chá»‰nh
env = CustomEnvironment(
    num_shards=8,
    num_nodes_per_shard=15,
    max_steps=1000
)

# Táº¡o agent
agent = CustomAgent(
    state_space=env.state_size,
    action_space=env.action_dims,
    learning_rate=0.0005
)

# Táº¡o consensus
consensus = AdaptiveConsensus(num_shards=8)

# Thiáº¿t láº­p mÃ´ phá»ng
for episode in range(num_episodes):
    state = env.reset()
    for step in range(max_steps):
        action = agent.act(state)
        
        # Xá»­ lÃ½ consensus
        protocol = action[1]
        consensus.apply_protocol(action[0], protocol)
        
        # Thá»±c hiá»‡n hÃ nh Ä‘á»™ng
        next_state, reward, done, info = env.step(action)
        
        # Há»c tá»« kinh nghiá»‡m
        agent.step(state, action, reward, next_state, done)
        
        if done:
            break
        
        state = next_state
```

## PhÃ¢n tÃ­ch káº¿t quáº£

QTrust cung cáº¥p cÃ¡c cÃ´ng cá»¥ Ä‘á»ƒ phÃ¢n tÃ­ch káº¿t quáº£ mÃ´ phá»ng:

```python
from qtrust.utils.analysis import (
    load_simulation_results,
    compare_configurations,
    plot_performance_metrics,
    analyze_attack_resilience
)

# Táº£i káº¿t quáº£
results = load_simulation_results('results/summary_qtrust_full_20250323_063732.txt')

# So sÃ¡nh cáº¥u hÃ¬nh
comparison = compare_configurations([
    'results/summary_basic_20250323_063729.txt',
    'results/summary_adaptive_consensus_20250323_063730.txt',
    'results/summary_dqn_routing_20250323_063731.txt',
    'results/summary_qtrust_full_20250323_063732.txt'
])

# Váº½ biá»ƒu Ä‘á»“
plot_performance_metrics(comparison, save_path='charts/comparison.png')

# PhÃ¢n tÃ­ch kháº£ nÄƒng chá»‘ng táº¥n cÃ´ng
resilience = analyze_attack_resilience(
    'results_comparison/attack_comparison/attack_comparison_20250323_064020.txt'
)
```

## Trá»±c quan hÃ³a

QTrust há»— trá»£ nhiá»u cÃ´ng cá»¥ trá»±c quan hÃ³a:

```python
from qtrust.utils.visualization import (
    plot_network_topology,
    plot_shard_distribution,
    plot_training_progress,
    create_radar_chart,
    plot_attack_comparison
)

# Váº½ topology máº¡ng
plot_network_topology(network, shards, save_path='charts/topology.png')

# Váº½ phÃ¢n phá»‘i shard
plot_shard_distribution(shards, trust_scores, save_path='charts/distribution.png')

# Váº½ tiáº¿n trÃ¬nh huáº¥n luyá»‡n
plot_training_progress(training_rewards, save_path='charts/training.png')

# Táº¡o biá»ƒu Ä‘á»“ radar
create_radar_chart(
    ['Throughput', 'Latency', 'Security', 'Energy', 'Cross-shard Rate'],
    [metrics['basic'], metrics['qtrust']],
    ['Basic', 'QTrust'],
    save_path='charts/radar.png'
)

# Váº½ so sÃ¡nh kháº£ nÄƒng chá»‘ng táº¥n cÃ´ng
plot_attack_comparison(attack_results, save_path='charts/attack_comparison.png')
``` 